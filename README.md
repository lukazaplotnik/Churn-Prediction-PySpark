# Churn Prediction with PySpark

### Project Motivation

### Business Context
Sparkify is a fictitious music streaming service, created by Udacity to mimic the datasets generated by companies such as Spotify or Pandora. Millions of users play their favorite songs through such services on a daily basis, either through free tier that plays advertisements, or by using a premium subscription model which offers additional functionalities and is typicaly ad-free. Users can upgrade or downgrade their service any time, but also cancel it altogether, so it is very important to make sure the users like the service. Every time a user interacts with the service, whether playing songs, adding them to playlists, rating them with the thumbs down/up, adding a friend, logging in or out, changing settings, data is generated. All this logs contain key insights for helping the business understand whether the users are happy with the service.

In order to make sure the business stays on track with its (financial) goals, it is key to identify the users that are likely to churn, i.e. users who are at risk of downgrading from premium to free tier, or cancelling the service altogether. If businesses can accurately identify these users in advance before they leave they can offer them discounts and other similar incentives and save millions in revenues. It is a well known fact that it is more expensive to acquire a new customer than it is to retain an existing one.

### Project Overview
The aim of this project is to build and train a supervised machine learning model that would be able to accurately identify users who are likely to cancel the music streaming service (in both free and paid tier), based on the information contained in the user activity logs recorded within a given observation window. The data used for training is the simulated Sparkify activity data provided by Udacity. The project is carried out by leveraging the Apache Spark distributed cluster-computing framework capabilities, through Python API for Spark, PySpark. The initial stages of model development (data understanding, exploratory analysis, also model selection) are performed on a smaller dataset using Spark in local mode, whereas the entire Sparkify dataset is too big to be processed locally and therefore an Elastic MapReduce (EMR) cluster has been deployed on the AWS cloud to train the selected models.

The project consists of three main steps that can be summarized as follows:

**Data Understanding**
- loading the dataset
- inferring the meaning of different variables, their type, the values they can take, their distribution
- understanding the relationship between different columns
- identifying missing values, potential duplicates

**Feature Engineering and Exploratory Data Analysis**
- transforming the original dataset (one row per user log) to a dataset with per-user (one row per user) information or statistics, obtained through mapping (e.g. user's gender, length of observation period, most recent subscription level, etc.) or log aggregation (e.g. song/advertisement/thumbs up/home page count)
- engineering the features that will be used to identify churned users, e.g. aggregated statistic per unit of time, number of plan changes, songs played vs. total activity ratio, thumbs up vs. thumbs down ratio, activity trend, etc.
- analyzing the correlation between engineered features
- defining and calculating the binary response variable: __1__ - users who cancelled their subscription within the observation period, and **0** - users who kept the service throughout
- performing exploratory data analysis comparing the engineered statistics for users who stayed vs users who churned

**Modelling and Evaluation**
- defining a pipeline that scales the numeric features, combines all features in the right format and fits a selected binary classifier (logistic regression, random forest classifier and gradient boosting classifier have been used)
- splitting the dataset into train and test set
- model tuning using grid-search with cross validation for all different classifiers on the training data (using the smaller Sparkify dataset in Spark local mode)
- evaluating model performance in cross validation, and analyzing feature importances
- retraining the models that performed best on the small Sparkify dataset also on the EMR cluster
- evaluating model performance on the test set



### File Descriptions
- `sparkify_main.ipynb` : Jupyter notebook with all the analyses, modelling steps, code, results, visualizations, plus all supporting discussions and comprehensive documentation
- `sparkify_cluster.ipynb` : slim-down version of the main Jupyter notebook adapted to train selected models on the AWS EMR cluster

The full Sparkify dataset (12GB) used in this project is hosted on Udacity's publicly available Amazon S3 bucket: s3n://udacity-dsnd/sparkify/sparkify_event_data.json

The smaller version used for data exploration (123MB) is available under: s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json


### Results
The results and conclusions are best presented in the accompanying [Medium article](https://medium.com/@lukazaplotnik/TODO).


### Acknowledgments
The data used in this project has been simulated by Udacity to mimic the data generated by actual music streaming companies. I have to give credit to Udacity for designing projects that are as close as possible to real world scenarios and for preparing the students for the challenges they will face on the job.

### Libraries
- PySpark version 2.4.3 was used in this project, mainly `pyspark.sql` module for working with structured data and `pyspark.ml` that provides a set of high-level APIs to create practical machine learning pipelines
- no additional libraries have been used beyond the Anaconda distribution of Python, and there should be no issues running the code using Python versions 3.x
