# Churn Prediction with PySpark

### 1. Business Context
Sparkify is a fictitious music streaming service, created by Udacity to mimic the datasets generated by companies such as Spotify or Pandora. Millions of users play their favorite songs through such services on a daily basis, either through free tier that plays advertisements, or by using a premium subscription model which offers additional functionalities and is typically ad-free. Users can upgrade or downgrade their subscription plan any time, but also cancel it altogether, so it is very important to make sure the users like the service.

Every time a user interacts with a music streaming app, whether playing songs, adding them to playlists, rating them with the thumbs down/up, adding a friend, logging in or out, changing settings, data is generated. User activity logs contain key insights for helping the businesses understand whether the users are happy with the service.

In order to stay on track with its (financial) goals, it is key for a music streaming business to identify users that are likely to churn, i.e. users who are at risk of downgrading from premium to free tier, or cancelling the service. If a music streaming business accurately identifies such users in advance, they can offer them discounts or other similar incentives and save millions in revenues. It is a well-known fact that it is more expensive to acquire a new customer than it is to retain an existing one.

### 2. Project Overview
Our goal is to build and train a binary classifier that is able to accurately identify users (in both free and paid tier) who cancelled the Sparkify music streaming service, based on the patterns obtained from their past activity and interaction with the service. A successfully trained model could be used to identify users who are likely to churn in advance.

The data used for training is the simulated Sparkify activity data provided by Udacity. The project is carried out by leveraging the Apache Spark distributed cluster-computing framework capabilities, through Python API for Spark, PySpark. The entire model development process (data understanding, feature engineering, model selection) is performed on a subset (1/100) of the full Sparkify dataset, by using Spark in local mode. Models that perform well on the smaller dataset are then trained and tested also on the full Sparkify dataset. Due to its size it cannot be processed locally and therefore an Elastic MapReduce (EMR) cluster has been deployed on the AWS cloud to carry out the tasks.

The project consists of three main steps that can be summarized as follows:

**Data Understanding**
- loading the dataset
- inferring the meaning of different variables, their type, the values they can take, their distribution
- understanding the relationships between different columns
- identifying missing values, potential duplicates

**Feature Engineering and Exploratory Data Analysis**
- transforming the original dataset (one row per user log) to a dataset with user-level (one row per user) information or statistics, obtained through mapping (e.g. user's gender, start/end of the observation period, etc.) or aggregation (e.g. song count, advertisement count, etc.)
- engineering the features that will be used to identify churned users, e.g. aggregated statistic per unit of time, number of plan changes, songs played vs. total activity ratio, thumbs up vs. thumbs down ratio, activity trend, etc.
- defining and calculating the binary response variable: 1 - users who cancelled their subscription within the observation period, and 0 - users who kept the service throughout
- analyzing the correlation between engineered features
- performing exploratory data analysis comparing the engineered statistics for users who stayed vs users who churned

**Modelling and Evaluation**
- defining pipelines that combine: standardization of the numerical features, feature assembly, and a selected binary classifier (logistic regression, random forest classifier or gradient boosting classifier)
- splitting the dataset into train and test set
- pipeline training and tuning using grid-search with cross validation for all different classifiers on the training data
- analyzing model performance in cross validation (using AUC as metric) and extracting feature importances
- retraining the models that performed best on the full training set and evaluating model performance on the test set (using standard AUC and F1 metrics)

The above-listed steps describe the end-to-end model development process that was performed on the smaller Sparkify data-set. Note that good-performing models were ultimately trained and tested also on the full Sparkify data-set. This was done by simply rerunning the developed code on the full data-set, using an AWS EMR cluster.


### 3. File Descriptions
- `sparkify_main.ipynb` : Jupyter notebook with all the analyses, modelling steps, code, results, visualizations, plus all supporting discussions and comprehensive documentation
- `sparkify_cluster.ipynb` : slim-down version of the main Jupyter notebook adapted to train selected models on the AWS EMR cluster

The full Sparkify dataset (12GB) used in this project is hosted on Udacity's publicly available Amazon S3 bucket: `s3n://udacity-dsnd/sparkify/sparkify_event_data.json`

The smaller version used for data exploration (128MB) is available under: `s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json`


### 4. Results
The results and conclusions are best presented in the accompanying [Medium article](https://medium.com/@lukazaplotnik/sparkify-churn-prediction-with-pyspark-da50652f2afc).

### 5. Libraries
- PySpark version 2.4.3 was used in this project, primarily `pyspark.sql` module for working with structured data and `pyspark.ml` that provides a set of high-level APIs to create practical machine learning pipelines
- no additional libraries have been used beyond the Anaconda distribution of Python, and there should be no issues running the code using Python versions 3.x

### 6. Acknowledgments
The data used in this project has been simulated by Udacity to mimic the data generated by actual music streaming companies. I have to give credit to Udacity for designing projects that are as close as possible to real world scenarios, and for helping students be prepared for the challenges they will face on the job.
