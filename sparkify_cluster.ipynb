{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Sparkify - Churn Prediction with PySpark\n"}, {"metadata": {}, "cell_type": "markdown", "source": "## 1. Intro"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import datetime\nimport time\nimport numpy as np\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import udf, col, concat, count, lit, avg, lag, first, last, when\nfrom pyspark.sql.functions import min as Fmin, max as Fmax, sum as Fsum, round as Fround\n\nfrom pyspark.sql.types import IntegerType, DateType, TimestampType\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler, Normalizer, StandardScaler\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, Evaluator", "execution_count": 2, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Create a Spark Session\nspark = SparkSession \\\n.builder \\\n.appName('Sparkify') \\\n.getOrCreate()", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## 2. Data Understanding"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Load data and store it into a Spark dataframe\n\n# Read in full sparkify dataset\nevent_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n#event_data = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\ndf = spark.read.json(event_data)", "execution_count": 4, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Structure of the dataframe\ndf.printSchema()", "execution_count": 4, "outputs": [{"output_type": "stream", "text": "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: long (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Shape of the smaller Sparkify dataframe\nnrows = df.count()\nncols = len(df.dtypes)", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(nrows)\nprint(ncols)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "26259199\n18", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Obtain the number of distinct users in the full Sparkify dataset\ndf.select(['userId']).dropDuplicates().count()", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "22278", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "# Obtain the distribution of the interaction types in the analyzed dataset\ndf.groupby('page').count().sort('count', ascending = False).show(22)", "execution_count": 8, "outputs": [{"output_type": "stream", "text": "+--------------------+--------+\n|                page|   count|\n+--------------------+--------+\n|            NextSong|20850272|\n|                Home| 1343102|\n|           Thumbs Up| 1151465|\n|     Add to Playlist|  597921|\n|         Roll Advert|  385212|\n|          Add Friend|  381664|\n|               Login|  296350|\n|              Logout|  296005|\n|         Thumbs Down|  239212|\n|           Downgrade|  184240|\n|                Help|  155100|\n|            Settings|  147074|\n|               About|   92759|\n|             Upgrade|   50507|\n|       Save Settings|   29516|\n|               Error|   25962|\n|      Submit Upgrade|   15135|\n|    Submit Downgrade|    6494|\n|              Cancel|    5003|\n|Cancellation Conf...|    5003|\n|            Register|     802|\n| Submit Registration|     401|\n+--------------------+--------+", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 3. Feature Engineering and Exploratory Data Analysis"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Default observation start and end timestamps for users who haven't registered after October 1, or churned\nobs_start_default = 1538352000000\nobs_end_default = 1543622400000", "execution_count": 5, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Lag the page column\nwindowsession = Window.partitionBy('sessionId').orderBy('ts')\ndf = df.withColumn(\"lagged_page\", lag(df.page).over(windowsession))", "execution_count": 6, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# All the values calculated here are the same for all logs belonging to a given user. Nevertheless, we attach them to the\n# original dataset, as they are needed for activity trend calculation \nwindowuser = Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n\n# Identify users that registered after the start of observation, and infer the start date accordingly\ndf = df.withColumn(\"beforefirstlog\", first(col('lagged_page')).over(windowuser))\ndf = df.withColumn(\"firstlogtime\", first(col('ts')).over(windowuser))\ndf = df.withColumn(\"obsstart\", \n                   when(df.beforefirstlog == \"Submit Registration\", df.firstlogtime).otherwise(obs_start_default))", "execution_count": 7, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Identify users that cancelled their service, i.e. users whose last log event is \"Cancellation Confirmation\", and\n# obtain the corresponding end of the observation period. This is done on the original dataset\n# so a single value, end of observation period, is copied to all rows belonging to a given userId.\n\ndf = df.withColumn(\"endstate\", last(col('page')).over(windowuser))\ndf = df.withColumn(\"lastlogtime\", last(col('ts')).over(windowuser))\ndf = df.withColumn(\"obsend\", when(df.endstate == \"Cancellation Confirmation\", df.lastlogtime).otherwise(obs_end_default))", "execution_count": 8, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# For each log compute the time from the beginning of observation...\ndf = df.withColumn(\"timefromstart\", col('ts')-col(\"obsstart\"))\n# ...and time before the end of observation\ndf = df.withColumn(\"timebeforeend\", col('obsend')-col('ts'))", "execution_count": 9, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Obtain user's last subscription level and add it to the original dataset\ndf = df.withColumn(\"lastlevel\", last(col('level')).over(windowuser))", "execution_count": 10, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Remove rows with missing userId \ndf = df.where(df.userId != \"\")", "execution_count": 11, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Remove rows with corrupted timestamp\ndf = df.where(df.ts < obs_end_default)", "execution_count": 12, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Length of the estimation window for activity trend calculation\ntrend_est_days = 14\ntrend_est_hours = trend_est_days * 24\n# In timestamp format\ntrend_est = trend_est_days * 24 * 60 * 60 * 1000", "execution_count": 13, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Aggregation by userId\ndf_user = df.groupby('userId')\\\n.agg(\n     # User-level features\n     first(when(col('lastlevel') == 'paid', 1).otherwise(0)).alias('lastlevel'),\n     first(when(col('gender') == \"F\", 1).otherwise(0)).alias('gender'),\n     first(col('obsstart')).alias('obsstart'),\n     first(col('obsend')).alias('obsend'),\n     first(col('endstate')).alias('endstate'),\n     #first(col('location')).alias('location'),\n     #first(col('userAgent')).alias('userAgent')\n     #first(col('registration')).alias('registration'),\n     \n     # Aggregated activity statistics\n     count(col('page')).alias('nact'),\n     Fsum(when(col('page') == \"NextSong\", 1).otherwise(0)).alias(\"nsongs\"),\n     Fsum(when(col('page') == \"Thumbs Up\", 1).otherwise(0)).alias(\"ntbup\"),\n     Fsum(when(col('page') == \"Thumbs Down\", 1).otherwise(0)).alias(\"ntbdown\"),\n     Fsum(when(col('page') == \"Add Friend\", 1).otherwise(0)).alias(\"nfriend\"),\n     Fsum(when(col('page') == \"Add to Playlist\", 1).otherwise(0)).alias(\"nplaylist\"),     \n     Fsum(when(col('page') == \"Submit Downgrade\", 1).otherwise(0)).alias(\"ndgrade\"),\n     Fsum(when(col('page') == \"Submit Upgrade\", 1).otherwise(0)).alias(\"nugrade\"),\n     Fsum(when(col('page') == \"Home\", 1).otherwise(0)).alias(\"nhome\"),\n     Fsum(when(col('page') == \"Roll Advert\", 1).otherwise(0)).alias(\"nadvert\"),\n     Fsum(when(col('page') == \"Help\", 1).otherwise(0)).alias(\"nhelp\"),\n     Fsum(when(col('page') == \"Settings\", 1).otherwise(0)).alias(\"nsettings\"),\n     Fsum(when(col('page') == \"Error\", 1).otherwise(0)).alias(\"nerror\"),\n     \n     # Aggregated activity statistics in different periods \n     Fsum(when(col('timebeforeend') < trend_est, 1).otherwise(0)).alias(\"nact_recent\"),\n     Fsum(when(col('timefromstart') < trend_est, 1).otherwise(0)).alias(\"nact_oldest\"),\n     Fsum(when((col('page') == \"NextSong\") & (col('timebeforeend') < trend_est), 1).otherwise(0)).alias(\"nsongs_recent\"),\n     Fsum(when((col('page') == \"NextSong\") & (col('timefromstart') < trend_est), 1).otherwise(0)).alias(\"nsongs_oldest\") )", "execution_count": 14, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Calculation of the defined features that will be used for identifying churned users\n# The first added column, 'obshours', represents the length of the user-specific observation period in hours. The column \n# is not one of the features but is used to calculate all aggregated statistics per hour\ndf_user = df_user.withColumn('obshours', (col('obsend') - col('obsstart'))/1000/3600)\\\n.withColumn('nact_perh', col('nact') / col('obshours'))\\\n.withColumn('nsongs_perh', col('nsongs') / col('obshours'))\\\n.withColumn('ntbup_perh', col('ntbup') / col('obshours'))\\\n.withColumn('ntbdown_perh', col('ntbdown') / col('obshours'))\\\n.withColumn('nfriend_perh', col('nfriend') / col('obshours'))\\\n.withColumn('nplaylist_perh', col('nplaylist') / col('obshours'))\\\n.withColumn('nhome_perh', col('nhome') / col('obshours'))\\\n.withColumn('nadvert_perh', col('nadvert') / col('obshours'))\\\n.withColumn('nerror_perh', col('nerror') / col('obshours'))\\\n.withColumn('upgradedowngrade', col('nugrade') + col('ndgrade'))\\\n.withColumn('songratio', col('nsongs') / col('nact'))\\\n.withColumn('positiveratio', (col('ntbup') + col('nfriend') + col('nplaylist')) / col('nact'))\\\n.withColumn('negativeratio', (col('ntbdown') + col('nhelp') + col('nerror') + col('nsettings')) / col('nact'))\\\n.withColumn('updownratio', col('ntbup') / (col('ntbdown') + 0.1))\\\n.withColumn('nact_recent_perh', col('nact_recent') / trend_est_hours)\\\n.withColumn('nact_oldest_perh', col('nact_oldest') / trend_est_hours)\\\n.withColumn('nsongs_recent_perh', col('nsongs_recent') / trend_est_hours)\\\n.withColumn('nsongs_oldest_perh', col('nsongs_oldest') / trend_est_hours)\\\n.withColumn('trend_act', (col('nact_recent_perh') - col('nact_oldest_perh')) / col('obshours'))\\\n.withColumn('trend_songs', (col('nsongs_recent_perh') - col('nsongs_oldest_perh')) / col('obshours'))\n#.withColumn('timesincereg', (col('obsstart') - col('registration'))/1000/3600)\\", "execution_count": 15, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Calculation of user's average number of items per session\nsession_avgnitems = df.groupby(['userId', 'sessionId'])\\\n.agg(Fmax(col('itemInSession')).alias('nitems'))\\\n.groupby('userId').agg(avg(col('nitems')).alias('avgsessionitems'))", "execution_count": 16, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Calculation of user's average session length\nsession_avglength = df.groupby(['userId', 'sessionId'])\\\n.agg(Fmin(col('ts')).alias('startsession'), Fmax(col('ts')).alias('endsession'))\\\n.groupby('userId').agg(avg(col('endsession')-col('startsession')).alias('avgsessionlength'))", "execution_count": 17, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Calculations to obtain user's average number of songs played between home visits\nwindowhome = (Window.partitionBy('userId').orderBy('ts').rangeBetween(Window.unboundedPreceding, 0))\ndf = df.withColumn(\"phasehome\", Fsum(when(df.page == \"Home\", 1).otherwise(0)).over(windowhome))\n\nsongs_home = df.groupby(['userId', 'phasehome'])\\\n.agg(Fsum(when(col('page') == \"NextSong\", 1).otherwise(0)).alias('total'))\\\n.groupby('userId').agg(avg(col('total')).alias('avgsongs'))", "execution_count": 18, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Adding the extra features to the df_user dataset with all other engineered features\ndf_user = df_user\\\n.join(session_avgnitems, on = 'userId')\\\n.join(session_avglength, on = 'userId')\\\n.join(songs_home, on = 'userId')", "execution_count": 19, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Calculating the binary response variable\ndf_user = df_user.withColumn('label', when(df_user.endstate == \"Cancellation Confirmation\", 1).otherwise(0))", "execution_count": 20, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Distribution of the binary response variable\ndf_user.groupby('label').count().show()", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "+-----+-----+\n|label|count|\n+-----+-----+\n|    1| 5003|\n|    0|17275|\n+-----+-----+", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Schema of the transformed dataset\ndf_user.printSchema()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "root\n |-- userId: string (nullable = true)\n |-- lastlevel: integer (nullable = true)\n |-- gender: integer (nullable = true)\n |-- obsstart: long (nullable = true)\n |-- obsend: long (nullable = true)\n |-- endstate: string (nullable = true)\n |-- nact: long (nullable = false)\n |-- nsongs: long (nullable = true)\n |-- ntbup: long (nullable = true)\n |-- ntbdown: long (nullable = true)\n |-- nfriend: long (nullable = true)\n |-- nplaylist: long (nullable = true)\n |-- ndgrade: long (nullable = true)\n |-- nugrade: long (nullable = true)\n |-- nhome: long (nullable = true)\n |-- nadvert: long (nullable = true)\n |-- nhelp: long (nullable = true)\n |-- nsettings: long (nullable = true)\n |-- nerror: long (nullable = true)\n |-- nact_recent: long (nullable = true)\n |-- nact_oldest: long (nullable = true)\n |-- nsongs_recent: long (nullable = true)\n |-- nsongs_oldest: long (nullable = true)\n |-- obshours: double (nullable = true)\n |-- nact_perh: double (nullable = true)\n |-- nsongs_perh: double (nullable = true)\n |-- ntbup_perh: double (nullable = true)\n |-- ntbdown_perh: double (nullable = true)\n |-- nfriend_perh: double (nullable = true)\n |-- nplaylist_perh: double (nullable = true)\n |-- nhome_perh: double (nullable = true)\n |-- nadvert_perh: double (nullable = true)\n |-- nerror_perh: double (nullable = true)\n |-- upgradedowngrade: long (nullable = true)\n |-- songratio: double (nullable = true)\n |-- positiveratio: double (nullable = true)\n |-- negativeratio: double (nullable = true)\n |-- updownratio: double (nullable = true)\n |-- nact_recent_perh: double (nullable = true)\n |-- nact_oldest_perh: double (nullable = true)\n |-- nsongs_recent_perh: double (nullable = true)\n |-- nsongs_oldest_perh: double (nullable = true)\n |-- trend_act: double (nullable = true)\n |-- trend_songs: double (nullable = true)\n |-- avgsessionitems: double (nullable = true)\n |-- avgsessionlength: double (nullable = true)\n |-- avgsongs: double (nullable = true)\n |-- label: integer (nullable = false)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## 4. Modelling and Evaluation"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Packing together multiple features  using VectorAssembler\nnumeric_columns = ['nsongs_perh', 'ntbup_perh','ntbdown_perh', 'nfriend_perh', \n'nadvert_perh', 'nerror_perh', 'upgradedowngrade', 'songratio', 'positiveratio','negativeratio', \n'updownratio', 'trend_songs', 'avgsessionitems','avgsongs']\n\nnumeric_assembler = VectorAssembler(inputCols = numeric_columns, outputCol = \"numericvectorized\")\n\n# Standardizing all numerical features at the same time\nscaler = StandardScaler(inputCol = \"numericvectorized\", outputCol = \"numericscaled\", withStd = True, withMean = True)\n#scaler = Normalizer(inputCol=\"numericvectorized\", outputCol=\"numericscaled\")\n\n# Adding the two binary features\nbinary_columns = ['lastlevel', 'gender']\ntotal_assembler = VectorAssembler(inputCols = binary_columns + [\"numericscaled\"], outputCol = \"features\")", "execution_count": 21, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "#Custom F1 score evaluator that we can use instead of BinaryClassificationEvaluator() in grid search\nclass F1score(Evaluator):\n\n    def __init__(self, predictionCol = \"prediction\", labelCol=\"label\"):\n        self.predictionCol = predictionCol\n        self.labelCol = labelCol\n\n    def _evaluate(self, dataset):\n        \n        # Calculate F1 score \n        tp = dataset.where((dataset.label == 1) & (dataset.prediction == 1)).count()\n        fp = dataset.where((dataset.label == 0) & (dataset.prediction == 1)).count()\n        tn = dataset.where((dataset.label == 0) & (dataset.prediction == 0)).count()\n        fn = dataset.where((dataset.label == 1) & (dataset.prediction == 0)).count()\n        \n        # Add epsilon to prevent division by zero\n        precision = tp / float(tp + fp + 0.00001)\n        recall = tp / float(tp + fn + 0.00001)\n        \n        f1 = 2 * precision * recall / float(precision + recall + 0.00001)\n        \n        return f1\n\n    def isLargerBetter(self):\n        return True", "execution_count": 22, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Split the data into training (plus validation) and test set \n\ntrain_plus_val, test = df_user.randomSplit([0.8, 0.2], seed = 9) \n#ntotal = df_user.count()\n#ntrainval = train_plus_val.count()\n#ntest = ntotal - ntrainval", "execution_count": 23, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print(ntotal)\nprint(ntrainval)\nprint(ntest)", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "22278\n17799\n4479", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Defining three different pipelines with three different classifiers, all with default parameters\n\n# Pipeline with logistic regression \n#lr =  LogisticRegression()\n#pipeline_lr = Pipeline(stages = [numeric_assembler, scaler, total_assembler, lr])\n\n# Pipeline with random forest classifier\n#rf = RandomForestClassifier()\n#pipeline_rf = Pipeline(stages = [numeric_assembler, scaler, total_assembler, rf])", "execution_count": 31, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Model gb1\n##### a) Training"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Perform the grid search by fitting the grid search object\ngb1 = GBTClassifier(maxDepth = 5, maxIter = 20)\npipeline_gb1 = Pipeline(stages = [numeric_assembler, scaler, total_assembler, gb1])\n\nstart = time.time()\nmodel_gb1 = pipeline_gb1.fit(train_plus_val)\nend = time.time()\nprint('Time spent for training:')\nprint(round(end-start))", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "Time spent for training:\n2510.0", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Display feature importances\nimportances = model_gb1.stages[-1].featureImportances\nimportances_list = [importances[i] for i in range(len(importances))]\nnames = binary_columns + numeric_columns\nprint(names)\nprint(importances_list)", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "['lastlevel', 'gender', 'nsongs_perh', 'ntbup_perh', 'ntbdown_perh', 'nfriend_perh', 'nadvert_perh', 'nerror_perh', 'upgradedowngrade', 'songratio', 'positiveratio', 'negativeratio', 'updownratio', 'trend_songs', 'avgsessionitems', 'avgsongs']\n[0.01774133563300751, 0.0007286232039209089, 0.06766983986664434, 0.0032197386588725114, 0.0914501514035554, 0.029716406544868613, 0.09803824638788945, 0.35941186881524695, 0.0221361649739096, 0.04981996403917844, 0.04519212423399973, 0.00869449171132609, 0.020951283491423053, 0.15103637455925087, 0.013870367363126513, 0.020323019113779733]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### b) Evaluation"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Obtain predictions on the test set\nresults_gb1 = model_gb1.transform(test)", "execution_count": 34, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# AUC score on the test set\nauc_evaluator = BinaryClassificationEvaluator()\nauc_gb1 = auc_evaluator.evaluate(results_gb1)\nprint('Model gb1 AUC score:')\nprint(auc_gb1)", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "Model gb1 AUC score:\n0.94108963301", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# F1 score on the test set\nf1_evaluator = F1score()\nf1score_gb1 = f1_evaluator._evaluate(results_gb1)\nprint('Model gb1 F1 score:')\nprint(f1score_gb1)", "execution_count": 36, "outputs": [{"output_type": "stream", "text": "Model gb1 F1 score:\n0.733178914983", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Model gb2"}, {"metadata": {}, "cell_type": "markdown", "source": "deleted"}, {"metadata": {}, "cell_type": "markdown", "source": "## Model gb3\n##### a) Training"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Perform the grid search by fitting the grid search object\ngb3 = GBTClassifier(maxDepth = 5, maxIter = 100)\npipeline_gb3 = Pipeline(stages = [numeric_assembler, scaler, total_assembler, gb3])\n\nstart = time.time()\nmodel_gb3 = pipeline_gb3.fit(train_plus_val)\nend = time.time()\nprint('Time spent for training:')\nprint(round(end-start))", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "Time spent for training:\n3617.0", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Display feature importances\nimportances = model_gb3.stages[-1].featureImportances\nimportances_list = [importances[i] for i in range(len(importances))]\nnames = binary_columns + numeric_columns\nprint(names)\nprint(importances_list)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "['lastlevel', 'gender', 'nsongs_perh', 'ntbup_perh', 'ntbdown_perh', 'nfriend_perh', 'nadvert_perh', 'nerror_perh', 'upgradedowngrade', 'songratio', 'positiveratio', 'negativeratio', 'updownratio', 'trend_songs', 'avgsessionitems', 'avgsongs']\n[0.017222788613416473, 0.0029163879308429824, 0.05925252577088501, 0.017520451309692618, 0.11098751604022397, 0.057678068589259855, 0.10041157683301612, 0.2875956765143996, 0.024464368627375974, 0.05554570899833321, 0.04879222038442258, 0.03051230192101087, 0.02414824289663846, 0.0916044494021778, 0.04027769525506716, 0.031070020913237347]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### b) Evaluation"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Obtain predictions on the test set\nresults_gb3 = model_gb3.transform(test)", "execution_count": 25, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# AUC score on the test set\nauc_evaluator = BinaryClassificationEvaluator()\nauc_gb3 = auc_evaluator.evaluate(results_gb3)\nprint('Model gb3 AUC score:')\nprint(auc_gb3)", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Model gb3 AUC score:\n0.971231876951", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# F1 score on the test set\nf1_evaluator = F1score()\nf1score_gb3 = f1_evaluator._evaluate(results_gb3)\nprint('Model gb3 F1 score:')\nprint(f1score_gb3)", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Model gb3 F1 score:\n0.816124047938", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Model gb4\n##### a) Training"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Perform the grid search by fitting the grid search object\ngb4 = GBTClassifier(maxDepth = 5, maxIter = 200)\npipeline_gb4 = Pipeline(stages = [numeric_assembler, scaler, total_assembler, gb4])\n\nstart = time.time()\nmodel_gb4 = pipeline_gb4.fit(train_plus_val)\nend = time.time()\nprint('Time spent for training:')\nprint(round(end-start))", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "Time spent for training:\n5861.0", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Display feature importances\nimportances = model_gb4.stages[-1].featureImportances\nimportances_list = [importances[i] for i in range(len(importances))]\nnames = binary_columns + numeric_columns\nprint(names)\nprint(importances_list)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "['lastlevel', 'gender', 'nsongs_perh', 'ntbup_perh', 'ntbdown_perh', 'nfriend_perh', 'nadvert_perh', 'nerror_perh', 'upgradedowngrade', 'songratio', 'positiveratio', 'negativeratio', 'updownratio', 'trend_songs', 'avgsessionitems', 'avgsongs']\n[0.013795342305289867, 0.003419130795754195, 0.05192724418392687, 0.023604481206826188, 0.18329355168409747, 0.060501586172062846, 0.0974098898402886, 0.2110401549499677, 0.02286163968506689, 0.04730318423769473, 0.04232860002653574, 0.043054343894025765, 0.036773157537074976, 0.07858267078570629, 0.046534988450475566, 0.03757003424520641]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "##### b) Evaluation"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Obtain predictions on the test set\nresults_gb4 = model_gb4.transform(test)", "execution_count": 25, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# AUC score on the test set\nauc_evaluator = BinaryClassificationEvaluator()\nauc_gb4 = auc_evaluator.evaluate(results_gb4)\nprint('Model gb4 AUC score:')\nprint(auc_gb4)", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "Model gb4 AUC score:\n0.980815674238", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# F1 score on the test set\nf1_evaluator = F1score()\nf1score_gb4 = f1_evaluator._evaluate(results_gb4)\nprint('Model gb4 F1 score:')\nprint(f1score_gb4)", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "Model gb4 F1 score:\n0.855347479183", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}